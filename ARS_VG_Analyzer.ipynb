{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ApoorvSaxena0109/cli-2/blob/main/ARS_VG_Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lURu3Tp2r7j2"
      },
      "source": [
        "# ARS-VG Analyzer - Research Notebook\n",
        "\n",
        "**AEM-REM Substitution and Vulnerability Graph Analyzer**\n",
        "\n",
        "A forensic accounting prototype for detecting earnings manipulation through the integration of Large Language Models, Causal Graph Theory, and Adversarial Simulation.\n",
        "\n",
        "---\n",
        "\n",
        "**Authors:**\n",
        "- Primary Researcher: Apoorv\n",
        "- \n",
        "\n",
        "**Platform:** Google Colab Pro+ (A100 GPU recommended)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxnyf7v_r7j_"
      },
      "source": [
        "## Section 1: Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zoc7FO-0r7kC",
        "outputId": "1513c9e6-0670-4070-9d1c-2eb9eda94f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GPU VERIFICATION REPORT\n",
            "============================================================\n",
            "\n",
            "GPU Status: AVAILABLE\n",
            "\n",
            "GPU Details:\n",
            "   - Device Name: NVIDIA A100-SXM4-40GB\n",
            "   - Total Memory: 40960 MB\n",
            "   - Free Memory: 40506 MB\n",
            "   - Driver Version: 550.54.15\n",
            "\n",
            "A100 GPU Detected - Optimal for this notebook!\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# GPU Verification\n",
        "\"\"\"\n",
        "This cell verifies GPU availability and displays hardware information.\n",
        "For optimal performance, this notebook is designed for Google Colab Pro+ with A100 GPU.\n",
        "\"\"\"\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def verify_gpu():\n",
        "    gpu_info = {\n",
        "        'available': False, 'name': None, 'memory_total': None,\n",
        "        'memory_free': None, 'is_a100': False, 'cuda_version': None, 'driver_version': None\n",
        "    }\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['nvidia-smi', '--query-gpu=name,memory.total,memory.free,driver_version', '--format=csv,noheader,nounits'],\n",
        "            capture_output=True, text=True, timeout=10\n",
        "        )\n",
        "        if result.returncode == 0 and result.stdout.strip():\n",
        "            gpu_info['available'] = True\n",
        "            parts = result.stdout.strip().split(', ')\n",
        "            if len(parts) >= 4:\n",
        "                gpu_info['name'] = parts[0].strip()\n",
        "                gpu_info['memory_total'] = f\"{parts[1].strip()} MB\"\n",
        "                gpu_info['memory_free'] = f\"{parts[2].strip()} MB\"\n",
        "                gpu_info['driver_version'] = parts[3].strip()\n",
        "                if 'A100' in gpu_info['name']: gpu_info['is_a100'] = True\n",
        "            cuda_result = subprocess.run(['nvidia-smi', '--query-gpu=cuda_version', '--format=csv,noheader'], capture_output=True, text=True, timeout=10)\n",
        "            if cuda_result.returncode == 0: gpu_info['cuda_version'] = cuda_result.stdout.strip()\n",
        "    except: pass\n",
        "    return gpu_info\n",
        "\n",
        "def display_gpu_info(gpu_info):\n",
        "    print(\"=\" * 60)\n",
        "    print(\"GPU VERIFICATION REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    if gpu_info['available']:\n",
        "        print(f\"\\nGPU Status: AVAILABLE\\n\\nGPU Details:\")\n",
        "        print(f\"   - Device Name: {gpu_info['name']}\")\n",
        "        print(f\"   - Total Memory: {gpu_info['memory_total']}\")\n",
        "        print(f\"   - Free Memory: {gpu_info['memory_free']}\")\n",
        "        if gpu_info.get('driver_version'): print(f\"   - Driver Version: {gpu_info['driver_version']}\")\n",
        "        if gpu_info.get('cuda_version'): print(f\"   - CUDA Version: {gpu_info['cuda_version']}\")\n",
        "        if gpu_info['is_a100']:\n",
        "            print(f\"\\nA100 GPU Detected - Optimal for this notebook!\")\n",
        "        else:\n",
        "            print(f\"\\nGPU detected but not A100. A100 recommended.\")\n",
        "    else:\n",
        "        print(f\"\\nGPU Status: NOT AVAILABLE\\nRunning in CPU Mode\")\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "GPU_INFO = verify_gpu()\n",
        "display_gpu_info(GPU_INFO)\n",
        "GPU_AVAILABLE = GPU_INFO['available']\n",
        "IS_A100 = GPU_INFO['is_a100']\n",
        "GPU_NAME = GPU_INFO['name']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb_2pr6hr7kH",
        "outputId": "97cd2edf-33e0-46e7-eb6e-8426e73b431c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DEPENDENCY INSTALLATION\n",
            "============================================================\n",
            "\n",
            "Installing core packages...\n",
            "   [OK] pandas==2.1.3\n",
            "   [OK] numpy==1.26.2\n",
            "   [OK] scipy==1.11.4\n",
            "   [OK] networkx==3.2.1\n",
            "   [OK] pyvis==0.3.2\n",
            "   [OK] chromadb==0.4.18\n",
            "   [OK] sentence-transformers==2.2.2\n",
            "   [OK] gradio==4.8.0\n",
            "   [OK] requests==2.31.0\n",
            "   [OK] python-dotenv==1.0.0\n",
            "   [OK] tqdm==4.66.1\n",
            "\n",
            "Installing special packages...\n",
            "   [SKIP] unstructured[pdf]==0.10.30\n",
            "   [OK] ollama==0.1.2\n",
            "\n",
            "Installed: 12, Failed: 1\n",
            "\n",
            "VERIFYING IMPORTS...\n",
            "   [FAIL] pandas\n",
            "   [OK] numpy\n",
            "   [FAIL] scipy\n",
            "   [OK] networkx\n",
            "   [FAIL] chromadb\n",
            "   [FAIL] gradio\n",
            "   [OK] requests\n",
            "   [OK] tqdm\n"
          ]
        }
      ],
      "source": [
        "# Dependency Installation\n",
        "import subprocess, sys\n",
        "\n",
        "def install_dependencies():\n",
        "    print(\"=\" * 60 + \"\\nDEPENDENCY INSTALLATION\\n\" + \"=\" * 60)\n",
        "    core_packages = [\n",
        "        \"pandas==2.1.3\", \"numpy==1.26.2\", \"scipy==1.11.4\",\n",
        "        \"networkx==3.2.1\", \"pyvis==0.3.2\", \"chromadb==0.4.18\",\n",
        "        \"sentence-transformers==2.2.2\", \"gradio==4.8.0\",\n",
        "        \"requests==2.31.0\", \"python-dotenv==1.0.0\", \"tqdm==4.66.1\"\n",
        "    ]\n",
        "    special_packages = [(\"unstructured[pdf]==0.10.30\", \"PDF\"), (\"ollama==0.1.2\", \"LLM\")]\n",
        "    installed, failed = [], []\n",
        "    print(\"\\nInstalling core packages...\")\n",
        "    for pkg in core_packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "            installed.append(pkg.split(\"==\")[0])\n",
        "            print(f\"   [OK] {pkg}\")\n",
        "        except: failed.append(pkg); print(f\"   [FAIL] {pkg}\")\n",
        "    print(\"\\nInstalling special packages...\")\n",
        "    for pkg, desc in special_packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "            installed.append(pkg.split(\"==\")[0].split(\"[\")[0])\n",
        "            print(f\"   [OK] {pkg}\")\n",
        "        except: failed.append(pkg); print(f\"   [SKIP] {pkg}\")\n",
        "    print(f\"\\nInstalled: {len(installed)}, Failed: {len(failed)}\")\n",
        "    return installed, failed\n",
        "\n",
        "def verify_imports():\n",
        "    print(\"\\nVERIFYING IMPORTS...\")\n",
        "    imports = [\"pandas\", \"numpy\", \"scipy\", \"networkx\", \"chromadb\", \"gradio\", \"requests\", \"tqdm\"]\n",
        "    for m in imports:\n",
        "        try: exec(f\"import {m}\"); print(f\"   [OK] {m}\")\n",
        "        except: print(f\"   [FAIL] {m}\")\n",
        "\n",
        "INSTALLED_PACKAGES, FAILED_PACKAGES = install_dependencies()\n",
        "verify_imports()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skS5F4WOr7kJ",
        "outputId": "103ccd7d-30cf-4a81-f0c5-b4764c7bf661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment: Colab\n",
            "============================================================\n",
            "OLLAMA INSTALLATION\n",
            "============================================================\n",
            "\n",
            "Installing Ollama...\n",
            "   [OK] Installed\n",
            "\n",
            "Starting Ollama server...\n",
            "   [OK] Started (PID: 6899)\n"
          ]
        }
      ],
      "source": [
        "# Ollama Server Setup\n",
        "import subprocess, time, requests, os\n",
        "\n",
        "OLLAMA_HOST = \"127.0.0.1\"\n",
        "OLLAMA_PORT = 11434\n",
        "OLLAMA_URL = f\"http://{OLLAMA_HOST}:{OLLAMA_PORT}\"\n",
        "\n",
        "def is_colab():\n",
        "    try: import google.colab; return True\n",
        "    except: return False\n",
        "\n",
        "def install_ollama():\n",
        "    print(\"=\" * 60 + \"\\nOLLAMA INSTALLATION\\n\" + \"=\" * 60)\n",
        "    try:\n",
        "        r = subprocess.run(['ollama', '--version'], capture_output=True, text=True, timeout=10)\n",
        "        if r.returncode == 0: print(f\"\\nOllama installed: {r.stdout.strip()}\"); return True\n",
        "    except: pass\n",
        "    print(\"\\nInstalling Ollama...\")\n",
        "    try:\n",
        "        r = subprocess.run(\"curl -fsSL https://ollama.com/install.sh | sh\", shell=True, capture_output=True, text=True, timeout=300)\n",
        "        if r.returncode == 0: print(\"   [OK] Installed\"); return True\n",
        "    except: pass\n",
        "    return False\n",
        "\n",
        "def check_ollama_health():\n",
        "    try: return requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=5).status_code == 200\n",
        "    except: return False\n",
        "\n",
        "def start_ollama_server():\n",
        "    print(\"\\nStarting Ollama server...\")\n",
        "    if check_ollama_health(): print(\"   [OK] Already running\"); return True\n",
        "    try:\n",
        "        env = os.environ.copy(); env['OLLAMA_HOST'] = f\"{OLLAMA_HOST}:{OLLAMA_PORT}\"\n",
        "        p = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, env=env, start_new_session=True)\n",
        "        for i in range(30):\n",
        "            time.sleep(1)\n",
        "            if check_ollama_health(): print(f\"   [OK] Started (PID: {p.pid})\"); return True\n",
        "    except: pass\n",
        "    return False\n",
        "\n",
        "print(f\"Environment: {'Colab' if is_colab() else 'Local'}\")\n",
        "OLLAMA_INSTALLED = install_ollama()\n",
        "OLLAMA_RUNNING = start_ollama_server() if OLLAMA_INSTALLED else False\n",
        "OLLAMA_AVAILABLE = OLLAMA_RUNNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORTx1sqAr7kM",
        "outputId": "dd7b325f-2e1a-4874-b868-8b453d3ca3f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DEEPSEEK MODEL DOWNLOAD\n",
            "Model: deepseek-r1:32b\n",
            "============================================================\n",
            "\n",
            "Downloading deepseek-r1:32b...\n",
            "   Progress: 10%\n",
            "   Progress: 20%\n",
            "   Progress: 30%\n",
            "   Progress: 40%\n",
            "   Progress: 50%\n",
            "   Progress: 60%\n",
            "   Progress: 71%\n",
            "   Progress: 81%\n",
            "   Progress: 91%\n",
            "\n",
            "[SUCCESS] Downloaded!\n",
            "\n",
            "Testing model...\n",
            "   Response: \n"
          ]
        }
      ],
      "source": [
        "# DeepSeek Model Download\n",
        "import requests, time, json\n",
        "\n",
        "MODEL_NAME = \"deepseek-r1:32b\"\n",
        "OLLAMA_URL = \"http://127.0.0.1:11434\"\n",
        "\n",
        "def check_model_exists(name):\n",
        "    try:\n",
        "        r = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=10)\n",
        "        if r.status_code == 200:\n",
        "            for m in r.json().get('models', []):\n",
        "                if m.get('name', '').startswith(name.split(':')[0]): return True\n",
        "    except: pass\n",
        "    return False\n",
        "\n",
        "def download_model(name):\n",
        "    print(\"=\" * 60 + f\"\\nDEEPSEEK MODEL DOWNLOAD\\nModel: {name}\\n\" + \"=\" * 60)\n",
        "    if check_model_exists(name): print(f\"\\n[OK] Already downloaded!\"); return True\n",
        "    print(f\"\\nDownloading {name}...\")\n",
        "    try:\n",
        "        r = requests.post(f\"{OLLAMA_URL}/api/pull\", json={\"name\": name, \"stream\": True}, stream=True, timeout=None)\n",
        "        if r.status_code != 200: return False\n",
        "        last_pct = 0\n",
        "        for line in r.iter_lines():\n",
        "            if line:\n",
        "                try:\n",
        "                    d = json.loads(line)\n",
        "                    if 'total' in d and 'completed' in d and d['total'] > 0:\n",
        "                        pct = (d['completed'] / d['total']) * 100\n",
        "                        if pct - last_pct >= 10:\n",
        "                            print(f\"   Progress: {pct:.0f}%\")\n",
        "                            last_pct = pct\n",
        "                except: pass\n",
        "        time.sleep(2)\n",
        "        if check_model_exists(name): print(\"\\n[SUCCESS] Downloaded!\"); return True\n",
        "    except Exception as e: print(f\"\\n[FAIL] {e}\")\n",
        "    return False\n",
        "\n",
        "def test_model(name):\n",
        "    print(\"\\nTesting model...\")\n",
        "    try:\n",
        "        r = requests.post(f\"{OLLAMA_URL}/api/generate\", json={\"model\": name, \"prompt\": \"What is 2+2?\", \"stream\": False, \"options\": {\"temperature\": 0, \"num_predict\": 10}}, timeout=60)\n",
        "        if r.status_code == 200:\n",
        "            ans = r.json().get('response', '').strip()\n",
        "            print(f\"   Response: {ans}\")\n",
        "            return '4' in ans\n",
        "    except: pass\n",
        "    return False\n",
        "\n",
        "MODEL_DOWNLOADED = download_model(MODEL_NAME)\n",
        "MODEL_READY = test_model(MODEL_NAME) if MODEL_DOWNLOADED else False\n",
        "DEEPSEEK_AVAILABLE = MODEL_READY\n",
        "DEEPSEEK_MODEL = MODEL_NAME if MODEL_READY else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dG9KMRJr7kS",
        "outputId": "e3041f99-f1e9-4454-86f1-55026bc0ef00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GOOGLE DRIVE MOUNTING\n",
            "============================================================\n",
            "\n",
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "\n",
            "[OK] Mounted!\n",
            "\n",
            "------------------------------------------------------------\n",
            "DIRECTORY STRUCTURE\n",
            "------------------------------------------------------------\n",
            "\n",
            "[OK] Base: /content/drive/MyDrive/ARS-VG-Analyzer\n",
            "   [OK] input/\n",
            "   [OK] processed/\n",
            "   [OK] chromadb/\n",
            "   [OK] results/\n",
            "   [OK] graphs/\n",
            "\n",
            "------------------------------------------------------------\n",
            "VERIFICATION\n",
            "------------------------------------------------------------\n",
            "   [OK] base: /content/drive/MyDrive/ARS-VG-Analyzer\n",
            "   [OK] input: /content/drive/MyDrive/ARS-VG-Analyzer/input\n",
            "   [OK] processed: /content/drive/MyDrive/ARS-VG-Analyzer/processed\n",
            "   [OK] chromadb: /content/drive/MyDrive/ARS-VG-Analyzer/chromadb\n",
            "   [OK] results: /content/drive/MyDrive/ARS-VG-Analyzer/results\n",
            "   [OK] graphs: /content/drive/MyDrive/ARS-VG-Analyzer/graphs\n",
            "\n",
            "Storage: Google Drive\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Google Drive Mounting and Directory Setup\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "BASE_DIR_NAME = \"ARS-VG-Analyzer\"\n",
        "SUBDIRECTORIES = [\"input\", \"processed\", \"chromadb\", \"results\", \"graphs\"]\n",
        "\n",
        "def is_colab():\n",
        "    try: import google.colab; return True\n",
        "    except: return False\n",
        "\n",
        "def mount_google_drive():\n",
        "    print(\"=\" * 60 + \"\\nGOOGLE DRIVE MOUNTING\\n\" + \"=\" * 60)\n",
        "    if not is_colab(): print(\"\\nNot in Colab. Using local storage.\"); return None\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive_path = Path(\"/content/drive/MyDrive\")\n",
        "        if drive_path.exists(): print(\"\\n[OK] Already mounted!\"); return str(drive_path)\n",
        "        print(\"\\nMounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "        if drive_path.exists(): print(\"\\n[OK] Mounted!\"); return str(drive_path)\n",
        "    except Exception as e: print(f\"\\n[FAIL] {e}\")\n",
        "    return None\n",
        "\n",
        "def create_directory_structure(base_path):\n",
        "    print(\"\\n\" + \"-\" * 60 + \"\\nDIRECTORY STRUCTURE\\n\" + \"-\" * 60)\n",
        "    analyzer_dir = Path(base_path) / BASE_DIR_NAME\n",
        "    paths = {\"base\": str(analyzer_dir)}\n",
        "    try: analyzer_dir.mkdir(parents=True, exist_ok=True); print(f\"\\n[OK] Base: {analyzer_dir}\")\n",
        "    except Exception as e: print(f\"\\n[FAIL] {e}\"); return None\n",
        "    for subdir in SUBDIRECTORIES:\n",
        "        try:\n",
        "            (analyzer_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
        "            paths[subdir] = str(analyzer_dir / subdir)\n",
        "            print(f\"   [OK] {subdir}/\")\n",
        "        except: pass\n",
        "    return paths\n",
        "\n",
        "def verify_dirs(paths):\n",
        "    print(\"\\n\" + \"-\" * 60 + \"\\nVERIFICATION\\n\" + \"-\" * 60)\n",
        "    for name, path in paths.items():\n",
        "        status = \"[OK]\" if Path(path).exists() else \"[FAIL]\"\n",
        "        print(f\"   {status} {name}: {path}\")\n",
        "    return all(Path(p).exists() for p in paths.values())\n",
        "\n",
        "# Main execution\n",
        "if is_colab():\n",
        "    DRIVE_PATH = mount_google_drive()\n",
        "    BASE_PATH = DRIVE_PATH if DRIVE_PATH else \"/content\"\n",
        "    DRIVE_MOUNTED = DRIVE_PATH is not None\n",
        "else:\n",
        "    print(\"=\" * 60 + \"\\nLOCAL MODE\\n\" + \"=\" * 60)\n",
        "    BASE_PATH = os.getcwd()\n",
        "    DRIVE_MOUNTED = False\n",
        "\n",
        "PATHS = create_directory_structure(BASE_PATH)\n",
        "DIRS_VALID = verify_dirs(PATHS) if PATHS else False\n",
        "\n",
        "print(f\"\\nStorage: {'Google Drive' if DRIVE_MOUNTED else 'Local'}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Export paths\n",
        "INPUT_DIR = PATHS.get(\"input\") if PATHS else None\n",
        "PROCESSED_DIR = PATHS.get(\"processed\") if PATHS else None\n",
        "CHROMADB_DIR = PATHS.get(\"chromadb\") if PATHS else None\n",
        "RESULTS_DIR = PATHS.get(\"results\") if PATHS else None\n",
        "GRAPHS_DIR = PATHS.get(\"graphs\") if PATHS else None\n",
        "BASE_DIR = PATHS.get(\"base\") if PATHS else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSQrc5A-r7kU"
      },
      "source": [
        "## Section 2: Data Structures and Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC14wtKpr7kW",
        "outputId": "a038dd92-62c2-416b-b01f-d376e3ffddb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CONFIGURATION SUMMARY\n",
            "============================================================\n",
            "\n",
            "Environment:\n",
            "   - Platform: Google Colab\n",
            "   - GPU Available: True\n",
            "   - Debug Mode: False\n",
            "\n",
            "LLM Configuration:\n",
            "   - Model: deepseek-r1:32b\n",
            "   - Ollama URL: http://127.0.0.1:11434\n",
            "   - Temperature: 0.1\n",
            "   - Max Tokens: 4096\n",
            "\n",
            "Embedding Configuration:\n",
            "   - Model: all-MiniLM-L6-v2\n",
            "   - Dimension: 384\n",
            "\n",
            "Chunking Configuration:\n",
            "   - Chunk Size: 1000\n",
            "   - Overlap: 200\n",
            "\n",
            "Analysis Thresholds:\n",
            "   - AEM Threshold: 0.65\n",
            "   - REM Threshold: 0.55\n",
            "   - Substitution Detection: 0.6\n",
            "\n",
            "Paths:\n",
            "   - base: /content/drive/MyDrive/ARS-VG-Analyzer\n",
            "   - input: /content/drive/MyDrive/ARS-VG-Analyzer/input\n",
            "   - processed: /content/drive/MyDrive/ARS-VG-Analyzer/processed\n",
            "   - chromadb: /content/drive/MyDrive/ARS-VG-Analyzer/chromadb\n",
            "   - results: /content/drive/MyDrive/ARS-VG-Analyzer/results\n",
            "   - graphs: /content/drive/MyDrive/ARS-VG-Analyzer/graphs\n",
            "\n",
            "============================================================\n",
            "\n",
            "[OK] Config instantiated successfully\n",
            "[OK] LLM URL: http://127.0.0.1:11434\n",
            "[OK] Environment: Colab\n"
          ]
        }
      ],
      "source": [
        "# Configuration Dataclasses\n",
        "\"\"\"\n",
        "Central configuration for the ARS-VG Analyzer using Python dataclasses.\n",
        "Provides sensible defaults, environment detection, and centralized settings management.\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import List, Optional, Dict, Any, Tuple, Literal\n",
        "from pathlib import Path\n",
        "import os\n",
        "import json\n",
        "\n",
        "def _is_colab() -> bool:\n",
        "    \"\"\"Check if running in Google Colab environment.\"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "@dataclass\n",
        "class LLMConfig:\n",
        "    \"\"\"Configuration for LLM (Ollama/DeepSeek) settings.\"\"\"\n",
        "    model_name: str = \"deepseek-r1:32b\"\n",
        "    ollama_host: str = \"127.0.0.1\"\n",
        "    ollama_port: int = 11434\n",
        "    temperature: float = 0.1\n",
        "    max_tokens: int = 4096\n",
        "    timeout: int = 120\n",
        "    num_ctx: int = 8192\n",
        "\n",
        "    @property\n",
        "    def ollama_url(self) -> str:\n",
        "        \"\"\"Get the full Ollama API URL.\"\"\"\n",
        "        return f\"http://{self.ollama_host}:{self.ollama_port}\"\n",
        "\n",
        "@dataclass\n",
        "class EmbeddingConfig:\n",
        "    \"\"\"Configuration for embedding model settings.\"\"\"\n",
        "    model_name: str = \"all-MiniLM-L6-v2\"\n",
        "    dimension: int = 384\n",
        "    batch_size: int = 32\n",
        "    normalize: bool = True\n",
        "\n",
        "@dataclass\n",
        "class ChunkingConfig:\n",
        "    \"\"\"Configuration for document chunking.\"\"\"\n",
        "    chunk_size: int = 1000\n",
        "    chunk_overlap: int = 200\n",
        "    min_chunk_length: int = 100\n",
        "    separator: str = \"\\n\\n\"\n",
        "\n",
        "@dataclass\n",
        "class GraphConfig:\n",
        "    \"\"\"Configuration for vulnerability graph construction.\"\"\"\n",
        "    max_nodes: int = 500\n",
        "    edge_threshold: float = 0.7\n",
        "    layout_algorithm: str = \"force_directed\"\n",
        "    node_size_range: Tuple[int, int] = (10, 50)\n",
        "    physics_enabled: bool = True\n",
        "    hierarchical: bool = False\n",
        "\n",
        "@dataclass\n",
        "class AnalysisConfig:\n",
        "    \"\"\"Configuration for AEM/REM analysis thresholds.\"\"\"\n",
        "    aem_threshold: float = 0.65\n",
        "    rem_threshold: float = 0.55\n",
        "    confidence_minimum: float = 0.5\n",
        "    substitution_detection_threshold: float = 0.6\n",
        "    max_iterations: int = 100\n",
        "    convergence_epsilon: float = 0.001\n",
        "\n",
        "@dataclass\n",
        "class PathConfig:\n",
        "    \"\"\"Configuration for file paths - initialized from global vars or defaults.\"\"\"\n",
        "    base_dir: str = \"\"\n",
        "    input_dir: str = \"\"\n",
        "    processed_dir: str = \"\"\n",
        "    chromadb_dir: str = \"\"\n",
        "    results_dir: str = \"\"\n",
        "    graphs_dir: str = \"\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Initialize paths from global variables or compute defaults.\"\"\"\n",
        "        g = globals()\n",
        "        if not self.base_dir:\n",
        "            self.base_dir = g.get('BASE_DIR') or os.getcwd()\n",
        "        if not self.input_dir:\n",
        "            self.input_dir = g.get('INPUT_DIR') or str(Path(self.base_dir) / 'input')\n",
        "        if not self.processed_dir:\n",
        "            self.processed_dir = g.get('PROCESSED_DIR') or str(Path(self.base_dir) / 'processed')\n",
        "        if not self.chromadb_dir:\n",
        "            self.chromadb_dir = g.get('CHROMADB_DIR') or str(Path(self.base_dir) / 'chromadb')\n",
        "        if not self.results_dir:\n",
        "            self.results_dir = g.get('RESULTS_DIR') or str(Path(self.base_dir) / 'results')\n",
        "        if not self.graphs_dir:\n",
        "            self.graphs_dir = g.get('GRAPHS_DIR') or str(Path(self.base_dir) / 'graphs')\n",
        "\n",
        "    def as_dict(self) -> Dict[str, str]:\n",
        "        \"\"\"Return all paths as a dictionary.\"\"\"\n",
        "        return {\n",
        "            'base': self.base_dir, 'input': self.input_dir, 'processed': self.processed_dir,\n",
        "            'chromadb': self.chromadb_dir, 'results': self.results_dir, 'graphs': self.graphs_dir\n",
        "        }\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Main configuration class combining all settings.\"\"\"\n",
        "    llm: LLMConfig = field(default_factory=LLMConfig)\n",
        "    embedding: EmbeddingConfig = field(default_factory=EmbeddingConfig)\n",
        "    chunking: ChunkingConfig = field(default_factory=ChunkingConfig)\n",
        "    graph: GraphConfig = field(default_factory=GraphConfig)\n",
        "    analysis: AnalysisConfig = field(default_factory=AnalysisConfig)\n",
        "    paths: PathConfig = field(default_factory=PathConfig)\n",
        "    is_colab: bool = field(default_factory=_is_colab)\n",
        "    gpu_available: bool = False\n",
        "    debug: bool = False\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Initialize GPU availability from global variables.\"\"\"\n",
        "        self.gpu_available = globals().get('GPU_AVAILABLE', False)\n",
        "\n",
        "    def display(self):\n",
        "        \"\"\"Display configuration summary.\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"CONFIGURATION SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"\\nEnvironment:\")\n",
        "        print(f\"   - Platform: {'Google Colab' if self.is_colab else 'Local'}\")\n",
        "        print(f\"   - GPU Available: {self.gpu_available}\")\n",
        "        print(f\"   - Debug Mode: {self.debug}\")\n",
        "        print(f\"\\nLLM Configuration:\")\n",
        "        print(f\"   - Model: {self.llm.model_name}\")\n",
        "        print(f\"   - Ollama URL: {self.llm.ollama_url}\")\n",
        "        print(f\"   - Temperature: {self.llm.temperature}\")\n",
        "        print(f\"   - Max Tokens: {self.llm.max_tokens}\")\n",
        "        print(f\"\\nEmbedding Configuration:\")\n",
        "        print(f\"   - Model: {self.embedding.model_name}\")\n",
        "        print(f\"   - Dimension: {self.embedding.dimension}\")\n",
        "        print(f\"\\nChunking Configuration:\")\n",
        "        print(f\"   - Chunk Size: {self.chunking.chunk_size}\")\n",
        "        print(f\"   - Overlap: {self.chunking.chunk_overlap}\")\n",
        "        print(f\"\\nAnalysis Thresholds:\")\n",
        "        print(f\"   - AEM Threshold: {self.analysis.aem_threshold}\")\n",
        "        print(f\"   - REM Threshold: {self.analysis.rem_threshold}\")\n",
        "        print(f\"   - Substitution Detection: {self.analysis.substitution_detection_threshold}\")\n",
        "        print(f\"\\nPaths:\")\n",
        "        for name, path in self.paths.as_dict().items():\n",
        "            print(f\"   - {name}: {path}\")\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Create and display global configuration instance\n",
        "CONFIG = Config()\n",
        "CONFIG.display()\n",
        "\n",
        "# Verify configuration is accessible\n",
        "print(f\"\\n[OK] Config instantiated successfully\")\n",
        "print(f\"[OK] LLM URL: {CONFIG.llm.ollama_url}\")\n",
        "print(f\"[OK] Environment: {'Colab' if CONFIG.is_colab else 'Local'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjLICp4jr7kY",
        "outputId": "73985847-f3ba-4ff9-dc04-ae3d81216138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "FINANCIAL DATA STRUCTURES TEST\n",
            "============================================================\n",
            "\n",
            "[OK] QuantitativeFact created:\n",
            "   - Account: Revenue\n",
            "   - Value: 1500.5 millions\n",
            "   - Scaled Value: $1,500,500,000\n",
            "   - Period: FY2024\n",
            "   - Footnotes: ['Note 2', 'Note 4']\n",
            "\n",
            "[OK] QualitativeClaim created:\n",
            "   - Section: MD&A\n",
            "   - Text length: 85 chars\n",
            "   - Embedded numbers: ['15%']\n",
            "\n",
            "[OK] GovernanceVector created:\n",
            "   - Auditor: Big4\n",
            "   - SOX Compliant: True\n",
            "   - Institutional Ownership: 65.5%\n",
            "\n",
            "[OK] JSON serialization works:\n",
            "   - QuantitativeFact JSON length: 233 chars\n",
            "   - Round-trip verified: True\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Financial Data Structures\n",
        "\"\"\"\n",
        "Core data structures for representing financial facts, claims, and governance data.\n",
        "These dataclasses form the canonical schema for the ARS-VG analysis pipeline.\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import List, Optional, Dict, Any, Literal\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "@dataclass\n",
        "class QuantitativeFact:\n",
        "    \"\"\"\n",
        "    Represents a quantitative financial fact extracted from financial statements.\n",
        "    Used for storing numerical data like Revenue, COGS, Inventory values.\n",
        "    \"\"\"\n",
        "    account_name: str  # e.g., \"Revenue\", \"Inventory\", \"Accounts Receivable\"\n",
        "    value: float  # The numerical value\n",
        "    period: str  # e.g., \"FY2024\", \"Q3-2024\"\n",
        "    currency: str = \"USD\"  # Currency code\n",
        "    source_table: str = \"\"  # Reference to source table in document\n",
        "    footnote_refs: List[str] = field(default_factory=list)  # References like [\"Note 4\", \"Note 7\"]\n",
        "    unit_scale: str = \"units\"  # \"thousands\", \"millions\", \"billions\", \"units\"\n",
        "    confidence: float = 1.0  # Extraction confidence score\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
        "        return asdict(self)\n",
        "\n",
        "    def to_json(self) -> str:\n",
        "        \"\"\"Convert to JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'QuantitativeFact':\n",
        "        \"\"\"Create instance from dictionary.\"\"\"\n",
        "        return cls(**data)\n",
        "\n",
        "    def scaled_value(self) -> float:\n",
        "        \"\"\"Return value adjusted by unit scale.\"\"\"\n",
        "        scale_map = {\"units\": 1, \"thousands\": 1e3, \"millions\": 1e6, \"billions\": 1e9}\n",
        "        return self.value * scale_map.get(self.unit_scale, 1)\n",
        "\n",
        "@dataclass\n",
        "class QualitativeClaim:\n",
        "    \"\"\"\n",
        "    Represents a qualitative claim from MD&A or notes sections.\n",
        "    Used for storing textual claims that need LLM evaluation.\n",
        "    \"\"\"\n",
        "    section: str  # e.g., \"MD&A\", \"Note 4\", \"Risk Factors\"\n",
        "    text: str  # The actual claim text\n",
        "    embedded_numbers: List[str] = field(default_factory=list)  # Numbers mentioned in text\n",
        "    sentiment_indicators: Dict[str, float] = field(default_factory=dict)  # e.g., {\"positive\": 0.7}\n",
        "    page_number: Optional[int] = None\n",
        "    paragraph_index: int = 0\n",
        "    related_accounts: List[str] = field(default_factory=list)  # Account names referenced\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
        "        return asdict(self)\n",
        "\n",
        "    def to_json(self) -> str:\n",
        "        \"\"\"Convert to JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'QualitativeClaim':\n",
        "        \"\"\"Create instance from dictionary.\"\"\"\n",
        "        return cls(**data)\n",
        "\n",
        "@dataclass\n",
        "class GovernanceVector:\n",
        "    \"\"\"\n",
        "    Represents governance and audit-related metadata.\n",
        "    Used for computing AEM/REM constraint scores.\n",
        "    \"\"\"\n",
        "    auditor_type: Literal[\"Big4\", \"Non-Big4\"] = \"Non-Big4\"\n",
        "    auditor_tenure: int = 0  # Years with current auditor\n",
        "    sox_compliant: bool = True  # SOX 404 compliance\n",
        "    institutional_ownership: float = 0.0  # Percentage (0-100)\n",
        "    analyst_coverage: int = 0  # Number of analysts\n",
        "    insider_ownership: float = 0.0  # Percentage (0-100)\n",
        "    board_independence: float = 0.0  # Percentage of independent directors\n",
        "    audit_committee_expertise: bool = False  # Financial expert on audit committee\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
        "        return asdict(self)\n",
        "\n",
        "    def to_json(self) -> str:\n",
        "        \"\"\"Convert to JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'GovernanceVector':\n",
        "        \"\"\"Create instance from dictionary.\"\"\"\n",
        "        return cls(**data)\n",
        "\n",
        "# Test the data structures\n",
        "print(\"=\" * 60)\n",
        "print(\"FINANCIAL DATA STRUCTURES TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test QuantitativeFact\n",
        "fact = QuantitativeFact(\n",
        "    account_name=\"Revenue\",\n",
        "    value=1500.5,\n",
        "    period=\"FY2024\",\n",
        "    currency=\"USD\",\n",
        "    source_table=\"Income Statement\",\n",
        "    footnote_refs=[\"Note 2\", \"Note 4\"],\n",
        "    unit_scale=\"millions\"\n",
        ")\n",
        "print(f\"\\n[OK] QuantitativeFact created:\")\n",
        "print(f\"   - Account: {fact.account_name}\")\n",
        "print(f\"   - Value: {fact.value} {fact.unit_scale}\")\n",
        "print(f\"   - Scaled Value: ${fact.scaled_value():,.0f}\")\n",
        "print(f\"   - Period: {fact.period}\")\n",
        "print(f\"   - Footnotes: {fact.footnote_refs}\")\n",
        "\n",
        "# Test QualitativeClaim\n",
        "claim = QualitativeClaim(\n",
        "    section=\"MD&A\",\n",
        "    text=\"Revenue growth of 15% was driven by strong performance in our cloud services segment.\",\n",
        "    embedded_numbers=[\"15%\"],\n",
        "    sentiment_indicators={\"positive\": 0.85, \"neutral\": 0.15},\n",
        "    related_accounts=[\"Revenue\", \"Cloud Services Revenue\"]\n",
        ")\n",
        "print(f\"\\n[OK] QualitativeClaim created:\")\n",
        "print(f\"   - Section: {claim.section}\")\n",
        "print(f\"   - Text length: {len(claim.text)} chars\")\n",
        "print(f\"   - Embedded numbers: {claim.embedded_numbers}\")\n",
        "\n",
        "# Test GovernanceVector\n",
        "governance = GovernanceVector(\n",
        "    auditor_type=\"Big4\",\n",
        "    auditor_tenure=5,\n",
        "    sox_compliant=True,\n",
        "    institutional_ownership=65.5,\n",
        "    analyst_coverage=12\n",
        ")\n",
        "print(f\"\\n[OK] GovernanceVector created:\")\n",
        "print(f\"   - Auditor: {governance.auditor_type}\")\n",
        "print(f\"   - SOX Compliant: {governance.sox_compliant}\")\n",
        "print(f\"   - Institutional Ownership: {governance.institutional_ownership}%\")\n",
        "\n",
        "# Test JSON serialization\n",
        "print(f\"\\n[OK] JSON serialization works:\")\n",
        "fact_json = fact.to_json()\n",
        "print(f\"   - QuantitativeFact JSON length: {len(fact_json)} chars\")\n",
        "\n",
        "# Test round-trip\n",
        "fact_restored = QuantitativeFact.from_dict(json.loads(fact_json))\n",
        "print(f\"   - Round-trip verified: {fact_restored.account_name == fact.account_name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60yXwSYzr7kb",
        "outputId": "7cb683e0-9479-4139-bfac-a73a93233686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GRAPH DATA STRUCTURES TEST\n",
            "============================================================\n",
            "\n",
            "[OK] FinancialNode (ACCOUNT) created:\n",
            "   - ID: revenue_fy2024\n",
            "   - Type: ACCOUNT\n",
            "   - Label: Revenue Fy2024\n",
            "   - Value: $1,500,000,000\n",
            "   - Size: 46\n",
            "\n",
            "[OK] FinancialNode (RATIO) created:\n",
            "   - ID: dso_fy2024\n",
            "   - Type: RATIO\n",
            "   - Value: 45.5 days\n",
            "   - Risk Score: 0.75\n",
            "   - Color: #ff4444\n",
            "\n",
            "[OK] FinancialEdge (IDENTITY) created:\n",
            "   - Source: revenue_fy2024\n",
            "   - Target: ar_fy2024\n",
            "   - Type: IDENTITY\n",
            "\n",
            "[OK] FinancialEdge (CORRELATION) with strain:\n",
            "   - Expected Ratio: 0.65\n",
            "   - Actual Ratio: 0.72\n",
            "   - Strain: 2.33 std devs\n",
            "   - Color: #ff0000\n",
            "\n",
            "[OK] Metadata dict handling:\n",
            "   - Node metadata: {'source': '10-K', 'audited': True}\n",
            "   - Edge metadata: {'equation': 'AR = Revenue * DSO/365'}\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Graph Data Structures - FinancialNode and FinancialEdge\n",
        "\"\"\"\n",
        "Data structures for the vulnerability graph representing financial statement relationships.\n",
        "Used by NetworkX and PyVis for graph analysis and visualization.\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import List, Optional, Dict, Any, Literal\n",
        "import json\n",
        "\n",
        "@dataclass\n",
        "class FinancialNode:\n",
        "    \"\"\"\n",
        "    Represents a node in the financial vulnerability graph.\n",
        "    Can represent an account (Revenue, COGS), a ratio (DSO, DIO), or governance metric.\n",
        "    \"\"\"\n",
        "    node_id: str  # Unique identifier e.g., \"revenue_fy2024\", \"dso_fy2024\"\n",
        "    node_type: Literal[\"ACCOUNT\", \"RATIO\", \"GOVERNANCE\"] = \"ACCOUNT\"\n",
        "    value: float = 0.0  # Current value\n",
        "    period: str = \"\"  # Time period e.g., \"FY2024\"\n",
        "    label: str = \"\"  # Display label\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)  # Additional attributes\n",
        "    risk_score: float = 0.0  # Calculated risk level (0-1)\n",
        "    category: str = \"\"  # Category e.g., \"Income Statement\", \"Balance Sheet\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Set default label if not provided.\"\"\"\n",
        "        if not self.label:\n",
        "            self.label = self.node_id.replace(\"_\", \" \").title()\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
        "        return asdict(self)\n",
        "\n",
        "    def to_json(self) -> str:\n",
        "        \"\"\"Convert to JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'FinancialNode':\n",
        "        \"\"\"Create instance from dictionary.\"\"\"\n",
        "        return cls(**data)\n",
        "\n",
        "    def get_color(self) -> str:\n",
        "        \"\"\"Get node color based on risk score.\"\"\"\n",
        "        if self.risk_score >= 0.7: return \"#ff4444\"  # Red - High risk\n",
        "        elif self.risk_score >= 0.4: return \"#ffaa00\"  # Orange - Medium risk\n",
        "        else: return \"#44aa44\"  # Green - Low risk\n",
        "\n",
        "    def get_size(self, min_size: int = 10, max_size: int = 50) -> int:\n",
        "        \"\"\"Get node size based on value magnitude.\"\"\"\n",
        "        if self.value == 0: return min_size\n",
        "        import math\n",
        "        log_val = math.log10(abs(self.value) + 1)\n",
        "        size = min_size + (max_size - min_size) * min(log_val / 10, 1)\n",
        "        return int(size)\n",
        "\n",
        "@dataclass\n",
        "class FinancialEdge:\n",
        "    \"\"\"\n",
        "    Represents an edge (relationship) in the financial vulnerability graph.\n",
        "    Types: IDENTITY (accounting equations), CORRELATION (statistical), REGULATORY (compliance).\n",
        "    \"\"\"\n",
        "    source: str  # Source node_id\n",
        "    target: str  # Target node_id\n",
        "    edge_type: Literal[\"IDENTITY\", \"CORRELATION\", \"REGULATORY\"] = \"CORRELATION\"\n",
        "    weight: float = 1.0  # Edge weight/strength\n",
        "    strain: Optional[float] = None  # Deviation from expected (None if not calculated)\n",
        "    expected_ratio: Optional[float] = None  # Expected relationship ratio\n",
        "    actual_ratio: Optional[float] = None  # Actual observed ratio\n",
        "    std_dev: Optional[float] = None  # Historical standard deviation\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
        "        return asdict(self)\n",
        "\n",
        "    def to_json(self) -> str:\n",
        "        \"\"\"Convert to JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'FinancialEdge':\n",
        "        \"\"\"Create instance from dictionary.\"\"\"\n",
        "        return cls(**data)\n",
        "\n",
        "    def calculate_strain(self) -> float:\n",
        "        \"\"\"Calculate strain if expected and actual ratios are available.\"\"\"\n",
        "        if self.expected_ratio is None or self.actual_ratio is None:\n",
        "            return 0.0\n",
        "        if self.std_dev and self.std_dev > 0:\n",
        "            self.strain = abs(self.actual_ratio - self.expected_ratio) / self.std_dev\n",
        "        else:\n",
        "            self.strain = abs(self.actual_ratio - self.expected_ratio)\n",
        "        return self.strain\n",
        "\n",
        "    def get_color(self) -> str:\n",
        "        \"\"\"Get edge color based on strain.\"\"\"\n",
        "        if self.strain is None: return \"#888888\"  # Gray - No strain calculated\n",
        "        if self.strain >= 2.0: return \"#ff0000\"  # Red - High strain\n",
        "        elif self.strain >= 1.0: return \"#ff8800\"  # Orange - Medium strain\n",
        "        else: return \"#00aa00\"  # Green - Low strain\n",
        "\n",
        "    def get_width(self, min_width: float = 1.0, max_width: float = 5.0) -> float:\n",
        "        \"\"\"Get edge width based on weight.\"\"\"\n",
        "        return min_width + (max_width - min_width) * min(self.weight, 1.0)\n",
        "\n",
        "# Test the graph data structures\n",
        "print(\"=\" * 60)\n",
        "print(\"GRAPH DATA STRUCTURES TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test FinancialNode with ACCOUNT type\n",
        "node_revenue = FinancialNode(\n",
        "    node_id=\"revenue_fy2024\",\n",
        "    node_type=\"ACCOUNT\",\n",
        "    value=1500000000,\n",
        "    period=\"FY2024\",\n",
        "    category=\"Income Statement\",\n",
        "    metadata={\"source\": \"10-K\", \"audited\": True}\n",
        ")\n",
        "print(f\"\\n[OK] FinancialNode (ACCOUNT) created:\")\n",
        "print(f\"   - ID: {node_revenue.node_id}\")\n",
        "print(f\"   - Type: {node_revenue.node_type}\")\n",
        "print(f\"   - Label: {node_revenue.label}\")\n",
        "print(f\"   - Value: ${node_revenue.value:,.0f}\")\n",
        "print(f\"   - Size: {node_revenue.get_size()}\")\n",
        "\n",
        "# Test FinancialNode with RATIO type\n",
        "node_dso = FinancialNode(\n",
        "    node_id=\"dso_fy2024\",\n",
        "    node_type=\"RATIO\",\n",
        "    value=45.5,\n",
        "    period=\"FY2024\",\n",
        "    label=\"Days Sales Outstanding\",\n",
        "    risk_score=0.75,\n",
        "    category=\"Efficiency Ratio\"\n",
        ")\n",
        "print(f\"\\n[OK] FinancialNode (RATIO) created:\")\n",
        "print(f\"   - ID: {node_dso.node_id}\")\n",
        "print(f\"   - Type: {node_dso.node_type}\")\n",
        "print(f\"   - Value: {node_dso.value} days\")\n",
        "print(f\"   - Risk Score: {node_dso.risk_score}\")\n",
        "print(f\"   - Color: {node_dso.get_color()}\")\n",
        "\n",
        "# Test FinancialEdge with IDENTITY type\n",
        "edge_identity = FinancialEdge(\n",
        "    source=\"revenue_fy2024\",\n",
        "    target=\"ar_fy2024\",\n",
        "    edge_type=\"IDENTITY\",\n",
        "    weight=1.0,\n",
        "    expected_ratio=1.0,\n",
        "    actual_ratio=0.08,\n",
        "    metadata={\"equation\": \"AR = Revenue * DSO/365\"}\n",
        ")\n",
        "print(f\"\\n[OK] FinancialEdge (IDENTITY) created:\")\n",
        "print(f\"   - Source: {edge_identity.source}\")\n",
        "print(f\"   - Target: {edge_identity.target}\")\n",
        "print(f\"   - Type: {edge_identity.edge_type}\")\n",
        "\n",
        "# Test FinancialEdge with CORRELATION type and strain\n",
        "edge_corr = FinancialEdge(\n",
        "    source=\"revenue_fy2024\",\n",
        "    target=\"cogs_fy2024\",\n",
        "    edge_type=\"CORRELATION\",\n",
        "    weight=0.85,\n",
        "    expected_ratio=0.65,\n",
        "    actual_ratio=0.72,\n",
        "    std_dev=0.03\n",
        ")\n",
        "strain = edge_corr.calculate_strain()\n",
        "print(f\"\\n[OK] FinancialEdge (CORRELATION) with strain:\")\n",
        "print(f\"   - Expected Ratio: {edge_corr.expected_ratio}\")\n",
        "print(f\"   - Actual Ratio: {edge_corr.actual_ratio}\")\n",
        "print(f\"   - Strain: {strain:.2f} std devs\")\n",
        "print(f\"   - Color: {edge_corr.get_color()}\")\n",
        "\n",
        "# Test metadata dict handling\n",
        "print(f\"\\n[OK] Metadata dict handling:\")\n",
        "print(f\"   - Node metadata: {node_revenue.metadata}\")\n",
        "print(f\"   - Edge metadata: {edge_identity.metadata}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KeZLvNKr7kc"
      },
      "source": [
        "## Section 3: Module 1 - Ingestion Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWEmS7Pfr7kf",
        "outputId": "de5135b9-7319-47e4-a51c-a963273c281d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "INGESTION SERVICE TEST\n",
            "============================================================\n",
            "\n",
            "[OK] Format Detection:\n",
            "   - PDF detection: N/A (no file)\n",
            "   - TXT detection: False\n",
            "\n",
            "[OK] File Validation:\n",
            "   - Invalid file handled: True\n",
            "   - Message: File not found: /nonexistent/file.pdf\n",
            "\n",
            "[OK] Text Chunking:\n",
            "   - Input length: 2200 chars\n",
            "   - Chunks created: 9\n",
            "   - First chunk size: 285 chars\n",
            "   - Chunk overlap working: True\n",
            "\n",
            "[OK] TextChunk dataclass:\n",
            "   - Content accessible: True\n",
            "   - Metadata dict: {}\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Ingestion Service\n",
        "\"\"\"\n",
        "Document ingestion pipeline for ARS-VG Analyzer.\n",
        "Handles PDF parsing, text extraction, chunking, and format detection.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional, Tuple, Literal\n",
        "from dataclasses import dataclass, field\n",
        "import re\n",
        "\n",
        "# Format detection\n",
        "SUPPORTED_FORMATS = [\"pdf\", \"txt\", \"csv\", \"xlsx\", \"html\"]\n",
        "\n",
        "def detect_format(file_path: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Detect document format from file extension and magic bytes.\n",
        "    Returns format string or None if unsupported.\n",
        "    \"\"\"\n",
        "    path = Path(file_path)\n",
        "    if not path.exists():\n",
        "        return None\n",
        "\n",
        "    # Check extension first\n",
        "    ext = path.suffix.lower().strip('.')\n",
        "    if ext in SUPPORTED_FORMATS:\n",
        "        return ext\n",
        "\n",
        "    # Check magic bytes for PDF\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            header = f.read(8)\n",
        "            if header.startswith(b'%PDF'):\n",
        "                return 'pdf'\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return None\n",
        "\n",
        "def validate_file(file_path: str) -> Tuple[bool, str]:\n",
        "    \"\"\"\n",
        "    Validate a file for processing.\n",
        "    Returns (is_valid, message).\n",
        "    \"\"\"\n",
        "    path = Path(file_path)\n",
        "\n",
        "    if not path.exists():\n",
        "        return False, f\"File not found: {file_path}\"\n",
        "\n",
        "    if not path.is_file():\n",
        "        return False, f\"Not a file: {file_path}\"\n",
        "\n",
        "    # Check file size (max 100MB)\n",
        "    size_mb = path.stat().st_size / (1024 * 1024)\n",
        "    if size_mb > 100:\n",
        "        return False, f\"File too large: {size_mb:.1f}MB (max 100MB)\"\n",
        "\n",
        "    fmt = detect_format(file_path)\n",
        "    if fmt is None:\n",
        "        return False, f\"Unsupported format: {path.suffix}\"\n",
        "\n",
        "    return True, f\"Valid {fmt.upper()} file ({size_mb:.2f}MB)\"\n",
        "\n",
        "@dataclass\n",
        "class TextChunk:\n",
        "    \"\"\"Represents a chunk of extracted text.\"\"\"\n",
        "    content: str\n",
        "    chunk_id: int\n",
        "    source_file: str\n",
        "    page_number: Optional[int] = None\n",
        "    section: str = \"\"\n",
        "    start_char: int = 0\n",
        "    end_char: int = 0\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.content)\n",
        "\n",
        "def extract_text_from_pdf(file_path: str) -> Tuple[str, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Extract text from PDF using unstructured library.\n",
        "    Returns (full_text, page_info_list).\n",
        "    \"\"\"\n",
        "    pages_info = []\n",
        "\n",
        "    try:\n",
        "        from unstructured.partition.pdf import partition_pdf\n",
        "        elements = partition_pdf(file_path)\n",
        "\n",
        "        full_text = \"\"\n",
        "        current_page = 1\n",
        "        page_text = \"\"\n",
        "\n",
        "        for elem in elements:\n",
        "            text = str(elem)\n",
        "            elem_page = getattr(elem.metadata, 'page_number', current_page) if hasattr(elem, 'metadata') else current_page\n",
        "\n",
        "            if elem_page != current_page:\n",
        "                if page_text.strip():\n",
        "                    pages_info.append({\"page\": current_page, \"text\": page_text.strip()})\n",
        "                page_text = \"\"\n",
        "                current_page = elem_page\n",
        "\n",
        "            page_text += text + \"\\n\"\n",
        "            full_text += text + \"\\n\"\n",
        "\n",
        "        # Add last page\n",
        "        if page_text.strip():\n",
        "            pages_info.append({\"page\": current_page, \"text\": page_text.strip()})\n",
        "\n",
        "        return full_text.strip(), pages_info\n",
        "\n",
        "    except ImportError:\n",
        "        # Fallback: try PyPDF2\n",
        "        try:\n",
        "            import PyPDF2\n",
        "            with open(file_path, 'rb') as f:\n",
        "                reader = PyPDF2.PdfReader(f)\n",
        "                full_text = \"\"\n",
        "                for i, page in enumerate(reader.pages):\n",
        "                    text = page.extract_text() or \"\"\n",
        "                    pages_info.append({\"page\": i+1, \"text\": text.strip()})\n",
        "                    full_text += text + \"\\n\"\n",
        "            return full_text.strip(), pages_info\n",
        "        except:\n",
        "            pass\n",
        "    except Exception as e:\n",
        "        print(f\"PDF extraction error: {e}\")\n",
        "\n",
        "    return \"\", []\n",
        "\n",
        "def extract_text_from_txt(file_path: str) -> str:\n",
        "    \"\"\"Extract text from plain text file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(file_path, 'r', encoding='latin-1') as f:\n",
        "            return f.read()\n",
        "\n",
        "def chunk_text(\n",
        "    text: str,\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 200,\n",
        "    source_file: str = \"\",\n",
        "    min_chunk_length: int = 100\n",
        ") -> List[TextChunk]:\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks.\n",
        "    \"\"\"\n",
        "    if not text or len(text) < min_chunk_length:\n",
        "        if text:\n",
        "            return [TextChunk(content=text, chunk_id=0, source_file=source_file, start_char=0, end_char=len(text))]\n",
        "        return []\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    chunk_id = 0\n",
        "\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "\n",
        "        # Try to break at sentence boundary\n",
        "        if end < len(text):\n",
        "            # Look for sentence endings\n",
        "            search_start = max(start + chunk_size - 100, start)\n",
        "            search_end = min(start + chunk_size + 100, len(text))\n",
        "            search_text = text[search_start:search_end]\n",
        "\n",
        "            # Find best break point\n",
        "            for pattern in ['. ', '.\\n', '! ', '? ', '\\n\\n']:\n",
        "                idx = search_text.rfind(pattern)\n",
        "                if idx > 0:\n",
        "                    end = search_start + idx + len(pattern)\n",
        "                    break\n",
        "        else:\n",
        "            end = len(text)\n",
        "\n",
        "        chunk_content = text[start:end].strip()\n",
        "\n",
        "        if len(chunk_content) >= min_chunk_length:\n",
        "            chunks.append(TextChunk(\n",
        "                content=chunk_content,\n",
        "                chunk_id=chunk_id,\n",
        "                source_file=source_file,\n",
        "                start_char=start,\n",
        "                end_char=end\n",
        "            ))\n",
        "            chunk_id += 1\n",
        "\n",
        "        # Move start position with overlap\n",
        "        start = end - chunk_overlap\n",
        "        if start >= len(text) - min_chunk_length:\n",
        "            break\n",
        "\n",
        "    return chunks\n",
        "\n",
        "@dataclass\n",
        "class ProcessedDocument:\n",
        "    \"\"\"Result of document processing.\"\"\"\n",
        "    file_path: str\n",
        "    format: str\n",
        "    full_text: str\n",
        "    chunks: List[TextChunk]\n",
        "    pages: List[Dict]\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "    success: bool = True\n",
        "    error: Optional[str] = None\n",
        "\n",
        "def process_document(\n",
        "    file_path: str,\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 200\n",
        ") -> ProcessedDocument:\n",
        "    \"\"\"\n",
        "    Main document processing function.\n",
        "    Detects format, extracts text, and creates chunks.\n",
        "    \"\"\"\n",
        "    # Validate file\n",
        "    is_valid, message = validate_file(file_path)\n",
        "    if not is_valid:\n",
        "        return ProcessedDocument(\n",
        "            file_path=file_path, format=\"unknown\", full_text=\"\",\n",
        "            chunks=[], pages=[], success=False, error=message\n",
        "        )\n",
        "\n",
        "    fmt = detect_format(file_path)\n",
        "    full_text = \"\"\n",
        "    pages = []\n",
        "\n",
        "    # Extract text based on format\n",
        "    if fmt == \"pdf\":\n",
        "        full_text, pages = extract_text_from_pdf(file_path)\n",
        "    elif fmt == \"txt\":\n",
        "        full_text = extract_text_from_txt(file_path)\n",
        "        pages = [{\"page\": 1, \"text\": full_text}]\n",
        "    else:\n",
        "        return ProcessedDocument(\n",
        "            file_path=file_path, format=fmt, full_text=\"\",\n",
        "            chunks=[], pages=[], success=False, error=f\"Format not yet supported: {fmt}\"\n",
        "        )\n",
        "\n",
        "    if not full_text:\n",
        "        return ProcessedDocument(\n",
        "            file_path=file_path, format=fmt, full_text=\"\",\n",
        "            chunks=[], pages=[], success=False, error=\"No text extracted\"\n",
        "        )\n",
        "\n",
        "    # Create chunks\n",
        "    chunks = chunk_text(full_text, chunk_size, chunk_overlap, file_path)\n",
        "\n",
        "    return ProcessedDocument(\n",
        "        file_path=file_path,\n",
        "        format=fmt,\n",
        "        full_text=full_text,\n",
        "        chunks=chunks,\n",
        "        pages=pages,\n",
        "        metadata={\n",
        "            \"char_count\": len(full_text),\n",
        "            \"chunk_count\": len(chunks),\n",
        "            \"page_count\": len(pages)\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Test the ingestion service\n",
        "print(\"=\" * 60)\n",
        "print(\"INGESTION SERVICE TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test format detection\n",
        "print(\"\\n[OK] Format Detection:\")\n",
        "print(f\"   - PDF detection: {detect_format('test.pdf') if detect_format('test.pdf') else 'N/A (no file)'}\")\n",
        "print(f\"   - TXT detection: {'txt' == detect_format.__code__.co_consts[0] if hasattr(detect_format, '__code__') else 'function works'}\")\n",
        "\n",
        "# Test file validation\n",
        "test_path = \"/nonexistent/file.pdf\"\n",
        "is_valid, msg = validate_file(test_path)\n",
        "print(f\"\\n[OK] File Validation:\")\n",
        "print(f\"   - Invalid file handled: {not is_valid}\")\n",
        "print(f\"   - Message: {msg}\")\n",
        "\n",
        "# Test chunking\n",
        "sample_text = \"This is sentence one. This is sentence two. \" * 50\n",
        "chunks = chunk_text(sample_text, chunk_size=200, chunk_overlap=50, source_file=\"test.txt\")\n",
        "print(f\"\\n[OK] Text Chunking:\")\n",
        "print(f\"   - Input length: {len(sample_text)} chars\")\n",
        "print(f\"   - Chunks created: {len(chunks)}\")\n",
        "if chunks:\n",
        "    print(f\"   - First chunk size: {len(chunks[0])} chars\")\n",
        "    print(f\"   - Chunk overlap working: {chunks[0].end_char > chunks[1].start_char if len(chunks) > 1 else 'N/A'}\")\n",
        "\n",
        "# Test TextChunk dataclass\n",
        "chunk = TextChunk(content=\"Test content\", chunk_id=0, source_file=\"test.pdf\", page_number=1)\n",
        "print(f\"\\n[OK] TextChunk dataclass:\")\n",
        "print(f\"   - Content accessible: {bool(chunk.content)}\")\n",
        "print(f\"   - Metadata dict: {chunk.metadata}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMPljGmKr7kh"
      },
      "source": [
        "## Section 4: Module 2 - Reasoning Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNPpqb93r7kh",
        "outputId": "93db721e-157d-40c1-a1db-9c1c995b59ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "REASONING SERVICE TEST\n",
            "============================================================\n",
            "\n",
            "[OK] OllamaClient created:\n",
            "   - Base URL: http://127.0.0.1:11434\n",
            "   - Model: deepseek-r1:32b\n",
            "   - Max retries: 3\n",
            "\n",
            "[OK] Connection Test:\n",
            "   - Server available: True\n",
            "\n",
            "[OK] Retry Logic:\n",
            "   - connect_with_retry method: True\n",
            "   - Retry delay: 2.0s\n",
            "\n",
            "[OK] ReasoningPrompt:\n",
            "   - Prompt length: 94 chars\n",
            "   - Contains task: True\n",
            "\n",
            "[OK] ReasoningService created:\n",
            "   - analyze_claim method: True\n",
            "   - evaluate_ratio_deviation method: True\n",
            "   - generate_substitution_hypothesis method: True\n",
            "\n",
            "[OK] Model Info:\n",
            "   - Name: deepseek-r1:32b\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Reasoning Service\n",
        "\"\"\"\n",
        "LLM-based reasoning service for ARS-VG Analyzer.\n",
        "Handles Ollama client, prompt generation, and response parsing.\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class OllamaClient:\n",
        "    \"\"\"Client for interacting with Ollama API.\"\"\"\n",
        "    host: str = \"127.0.0.1\"\n",
        "    port: int = 11434\n",
        "    model: str = \"deepseek-r1:32b\"\n",
        "    timeout: int = 120\n",
        "    max_retries: int = 3\n",
        "    retry_delay: float = 2.0\n",
        "\n",
        "    @property\n",
        "    def base_url(self) -> str:\n",
        "        return f\"http://{self.host}:{self.port}\"\n",
        "\n",
        "    def is_connected(self) -> bool:\n",
        "        \"\"\"Check if Ollama server is available.\"\"\"\n",
        "        try:\n",
        "            r = requests.get(f\"{self.base_url}/api/tags\", timeout=5)\n",
        "            return r.status_code == 200\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def connect_with_retry(self) -> bool:\n",
        "        \"\"\"Connect to Ollama with retry logic.\"\"\"\n",
        "        for attempt in range(self.max_retries):\n",
        "            if self.is_connected():\n",
        "                return True\n",
        "            if attempt < self.max_retries - 1:\n",
        "                time.sleep(self.retry_delay)\n",
        "        return False\n",
        "\n",
        "    def generate(self, prompt: str, temperature: float = 0.1, max_tokens: int = 4096) -> Tuple[str, bool]:\n",
        "        \"\"\"Generate response from LLM. Returns (response_text, success).\"\"\"\n",
        "        if not self.connect_with_retry():\n",
        "            return \"Error: Cannot connect to Ollama server\", False\n",
        "\n",
        "        try:\n",
        "            payload = {\n",
        "                \"model\": self.model,\n",
        "                \"prompt\": prompt,\n",
        "                \"stream\": False,\n",
        "                \"options\": {\n",
        "                    \"temperature\": temperature,\n",
        "                    \"num_predict\": max_tokens\n",
        "                }\n",
        "            }\n",
        "            r = requests.post(f\"{self.base_url}/api/generate\", json=payload, timeout=self.timeout)\n",
        "            if r.status_code == 200:\n",
        "                return r.json().get(\"response\", \"\"), True\n",
        "            return f\"Error: HTTP {r.status_code}\", False\n",
        "        except requests.Timeout:\n",
        "            return \"Error: Request timeout\", False\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\", False\n",
        "\n",
        "    def get_model_info(self) -> Optional[Dict]:\n",
        "        \"\"\"Get information about the current model.\"\"\"\n",
        "        try:\n",
        "            r = requests.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
        "            if r.status_code == 200:\n",
        "                for model in r.json().get(\"models\", []):\n",
        "                    if model.get(\"name\", \"\").startswith(self.model.split(\":\")[0]):\n",
        "                        return model\n",
        "        except:\n",
        "            pass\n",
        "        return None\n",
        "\n",
        "@dataclass\n",
        "class ReasoningPrompt:\n",
        "    \"\"\"Template for reasoning prompts.\"\"\"\n",
        "    system_context: str = \"\"\n",
        "    task: str = \"\"\n",
        "    data: str = \"\"\n",
        "    output_format: str = \"JSON\"\n",
        "\n",
        "    def build(self) -> str:\n",
        "        \"\"\"Build the full prompt string.\"\"\"\n",
        "        parts = []\n",
        "        if self.system_context:\n",
        "            parts.append(f\"Context: {self.system_context}\")\n",
        "        parts.append(f\"Task: {self.task}\")\n",
        "        if self.data:\n",
        "            parts.append(f\"Data:\\n{self.data}\")\n",
        "        parts.append(f\"Provide your response in {self.output_format} format.\")\n",
        "        return \"\\n\\n\".join(parts)\n",
        "\n",
        "class ReasoningService:\n",
        "    \"\"\"Service for LLM-based reasoning on financial data.\"\"\"\n",
        "\n",
        "    def __init__(self, client: Optional[OllamaClient] = None):\n",
        "        self.client = client or OllamaClient()\n",
        "        self._cache: Dict[str, str] = {}\n",
        "\n",
        "    def analyze_claim(self, claim_text: str, context: str = \"\") -> Dict[str, Any]:\n",
        "        \"\"\"Analyze a qualitative claim for manipulation indicators.\"\"\"\n",
        "        prompt = ReasoningPrompt(\n",
        "            system_context=\"You are a forensic accounting expert analyzing financial statements for earnings manipulation.\",\n",
        "            task=f\"Analyze this claim for potential manipulation indicators:\\n\\\"{claim_text}\\\"\",\n",
        "            data=context,\n",
        "            output_format=\"JSON with keys: credibility_score (0-1), red_flags (list), reasoning (string)\"\n",
        "        )\n",
        "\n",
        "        response, success = self.client.generate(prompt.build())\n",
        "        if not success:\n",
        "            return {\"error\": response, \"success\": False}\n",
        "\n",
        "        try:\n",
        "            # Try to parse JSON from response\n",
        "            json_start = response.find(\"{\")\n",
        "            json_end = response.rfind(\"}\") + 1\n",
        "            if json_start >= 0 and json_end > json_start:\n",
        "                return json.loads(response[json_start:json_end])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return {\"raw_response\": response, \"success\": True}\n",
        "\n",
        "    def evaluate_ratio_deviation(self, ratio_name: str, expected: float, actual: float, std_dev: float) -> Dict[str, Any]:\n",
        "        \"\"\"Evaluate whether a ratio deviation is suspicious.\"\"\"\n",
        "        deviation = abs(actual - expected) / std_dev if std_dev > 0 else abs(actual - expected)\n",
        "\n",
        "        prompt = ReasoningPrompt(\n",
        "            system_context=\"You are analyzing financial ratios for anomalies.\",\n",
        "            task=f\"Evaluate this ratio deviation: {ratio_name}\",\n",
        "            data=f\"Expected: {expected:.4f}, Actual: {actual:.4f}, Deviation: {deviation:.2f} std devs\",\n",
        "            output_format=\"JSON with keys: suspicious (bool), explanation (string), severity (low/medium/high)\"\n",
        "        )\n",
        "\n",
        "        response, success = self.client.generate(prompt.build(), temperature=0.1)\n",
        "        if not success:\n",
        "            return {\"error\": response, \"success\": False}\n",
        "\n",
        "        try:\n",
        "            json_start = response.find(\"{\")\n",
        "            json_end = response.rfind(\"}\") + 1\n",
        "            if json_start >= 0 and json_end > json_start:\n",
        "                return json.loads(response[json_start:json_end])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return {\"raw_response\": response, \"success\": True}\n",
        "\n",
        "    def generate_substitution_hypothesis(self, aem_indicators: List[str], rem_indicators: List[str]) -> Dict[str, Any]:\n",
        "        \"\"\"Generate hypothesis about AEM/REM substitution patterns.\"\"\"\n",
        "        prompt = ReasoningPrompt(\n",
        "            system_context=\"You are analyzing patterns of earnings manipulation.\",\n",
        "            task=\"Analyze the relationship between AEM and REM indicators to identify substitution patterns.\",\n",
        "            data=f\"AEM Indicators: {aem_indicators}\\nREM Indicators: {rem_indicators}\",\n",
        "            output_format=\"JSON with keys: substitution_detected (bool), pattern_type (string), confidence (0-1), explanation (string)\"\n",
        "        )\n",
        "\n",
        "        response, success = self.client.generate(prompt.build())\n",
        "        if not success:\n",
        "            return {\"error\": response, \"success\": False}\n",
        "\n",
        "        try:\n",
        "            json_start = response.find(\"{\")\n",
        "            json_end = response.rfind(\"}\") + 1\n",
        "            if json_start >= 0 and json_end > json_start:\n",
        "                return json.loads(response[json_start:json_end])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return {\"raw_response\": response, \"success\": True}\n",
        "\n",
        "# Test the reasoning service\n",
        "print(\"=\" * 60)\n",
        "print(\"REASONING SERVICE TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test OllamaClient\n",
        "client = OllamaClient()\n",
        "print(f\"\\n[OK] OllamaClient created:\")\n",
        "print(f\"   - Base URL: {client.base_url}\")\n",
        "print(f\"   - Model: {client.model}\")\n",
        "print(f\"   - Max retries: {client.max_retries}\")\n",
        "\n",
        "# Test connection\n",
        "is_connected = client.is_connected()\n",
        "print(f\"\\n[OK] Connection Test:\")\n",
        "print(f\"   - Server available: {is_connected}\")\n",
        "\n",
        "# Test retry logic\n",
        "print(f\"\\n[OK] Retry Logic:\")\n",
        "print(f\"   - connect_with_retry method: {callable(client.connect_with_retry)}\")\n",
        "print(f\"   - Retry delay: {client.retry_delay}s\")\n",
        "\n",
        "# Test ReasoningPrompt\n",
        "prompt = ReasoningPrompt(\n",
        "    system_context=\"Test context\",\n",
        "    task=\"Test task\",\n",
        "    data=\"Test data\",\n",
        "    output_format=\"JSON\"\n",
        ")\n",
        "built_prompt = prompt.build()\n",
        "print(f\"\\n[OK] ReasoningPrompt:\")\n",
        "print(f\"   - Prompt length: {len(built_prompt)} chars\")\n",
        "print(f\"   - Contains task: {'Test task' in built_prompt}\")\n",
        "\n",
        "# Test ReasoningService\n",
        "service = ReasoningService(client)\n",
        "print(f\"\\n[OK] ReasoningService created:\")\n",
        "print(f\"   - analyze_claim method: {callable(service.analyze_claim)}\")\n",
        "print(f\"   - evaluate_ratio_deviation method: {callable(service.evaluate_ratio_deviation)}\")\n",
        "print(f\"   - generate_substitution_hypothesis method: {callable(service.generate_substitution_hypothesis)}\")\n",
        "\n",
        "# Test model info (only if connected)\n",
        "if is_connected:\n",
        "    model_info = client.get_model_info()\n",
        "    if model_info:\n",
        "        print(f\"\\n[OK] Model Info:\")\n",
        "        print(f\"   - Name: {model_info.get('name', 'N/A')}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lHwL3j4r7ki",
        "outputId": "95566047-c257-48ab-acbb-f4d1c5103172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "VECTOR STORE TEST\n",
            "============================================================\n",
            "\n",
            "[OK] VectorStoreConfig created:\n",
            "   - Collection: ars_vg_documents\n",
            "   - Embedding model: all-MiniLM-L6-v2\n",
            "   - Persist directory: /content/drive/MyDrive/ARS-VG-Analyzer/chromadb\n",
            "\n",
            "[OK] VectorStore created:\n",
            "   - Initialized: False\n",
            "VectorStore initialization error: `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.\n",
            "\n",
            "[OK] VectorStore initialization:\n",
            "   - Success: False\n",
            "   - Is initialized: False\n",
            "\n",
            "[SKIP] Document operations (ChromaDB not available)\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Vector Store Service (ChromaDB)\n",
        "\"\"\"\n",
        "ChromaDB-based vector store for semantic search and retrieval.\n",
        "Handles document embeddings, storage, and similarity queries.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class VectorStoreConfig:\n",
        "    \"\"\"Configuration for vector store.\"\"\"\n",
        "    collection_name: str = \"ars_vg_documents\"\n",
        "    embedding_model: str = \"all-MiniLM-L6-v2\"\n",
        "    persist_directory: str = \"\"\n",
        "    distance_metric: str = \"cosine\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if not self.persist_directory:\n",
        "            self.persist_directory = globals().get('CHROMADB_DIR') or './chromadb'\n",
        "\n",
        "class VectorStore:\n",
        "    \"\"\"ChromaDB-based vector store for document embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Optional[VectorStoreConfig] = None):\n",
        "        self.config = config or VectorStoreConfig()\n",
        "        self._client = None\n",
        "        self._collection = None\n",
        "        self._embedding_fn = None\n",
        "        self._initialized = False\n",
        "\n",
        "    def initialize(self) -> bool:\n",
        "        \"\"\"Initialize ChromaDB client and collection.\"\"\"\n",
        "        try:\n",
        "            import chromadb\n",
        "            from chromadb.config import Settings\n",
        "\n",
        "            # Create persist directory if needed\n",
        "            persist_path = Path(self.config.persist_directory)\n",
        "            persist_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Initialize client with persistence\n",
        "            self._client = chromadb.Client(Settings(\n",
        "                chroma_db_impl=\"duckdb+parquet\",\n",
        "                persist_directory=str(persist_path),\n",
        "                anonymized_telemetry=False\n",
        "            ))\n",
        "\n",
        "            # Try to load embedding function\n",
        "            try:\n",
        "                from chromadb.utils import embedding_functions\n",
        "                self._embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "                    model_name=self.config.embedding_model\n",
        "                )\n",
        "            except:\n",
        "                self._embedding_fn = None\n",
        "\n",
        "            # Get or create collection\n",
        "            self._collection = self._client.get_or_create_collection(\n",
        "                name=self.config.collection_name,\n",
        "                embedding_function=self._embedding_fn,\n",
        "                metadata={\"hnsw:space\": self.config.distance_metric}\n",
        "            )\n",
        "\n",
        "            self._initialized = True\n",
        "            return True\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"ChromaDB not installed. Run: pip install chromadb\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"VectorStore initialization error: {e}\")\n",
        "            return False\n",
        "\n",
        "    @property\n",
        "    def is_initialized(self) -> bool:\n",
        "        return self._initialized and self._collection is not None\n",
        "\n",
        "    def add_documents(self, documents: List[str], metadatas: Optional[List[Dict]] = None, ids: Optional[List[str]] = None) -> bool:\n",
        "        \"\"\"Add documents to the collection.\"\"\"\n",
        "        if not self.is_initialized:\n",
        "            if not self.initialize():\n",
        "                return False\n",
        "\n",
        "        try:\n",
        "            # Generate IDs if not provided\n",
        "            if ids is None:\n",
        "                existing_count = self._collection.count()\n",
        "                ids = [f\"doc_{existing_count + i}\" for i in range(len(documents))]\n",
        "\n",
        "            # Add documents\n",
        "            self._collection.add(\n",
        "                documents=documents,\n",
        "                metadatas=metadatas or [{}] * len(documents),\n",
        "                ids=ids\n",
        "            )\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding documents: {e}\")\n",
        "            return False\n",
        "\n",
        "    def query(self, query_text: str, n_results: int = 5) -> Dict[str, Any]:\n",
        "        \"\"\"Query the collection for similar documents.\"\"\"\n",
        "        if not self.is_initialized:\n",
        "            if not self.initialize():\n",
        "                return {\"error\": \"Store not initialized\", \"documents\": [], \"distances\": []}\n",
        "\n",
        "        try:\n",
        "            results = self._collection.query(\n",
        "                query_texts=[query_text],\n",
        "                n_results=n_results\n",
        "            )\n",
        "            return {\n",
        "                \"documents\": results.get(\"documents\", [[]])[0],\n",
        "                \"metadatas\": results.get(\"metadatas\", [[]])[0],\n",
        "                \"distances\": results.get(\"distances\", [[]])[0],\n",
        "                \"ids\": results.get(\"ids\", [[]])[0]\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"documents\": [], \"distances\": []}\n",
        "\n",
        "    def count(self) -> int:\n",
        "        \"\"\"Get the number of documents in the collection.\"\"\"\n",
        "        if not self.is_initialized:\n",
        "            return 0\n",
        "        try:\n",
        "            return self._collection.count()\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    def persist(self) -> bool:\n",
        "        \"\"\"Persist the collection to disk.\"\"\"\n",
        "        if not self.is_initialized:\n",
        "            return False\n",
        "        try:\n",
        "            self._client.persist()\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def delete_collection(self) -> bool:\n",
        "        \"\"\"Delete the entire collection.\"\"\"\n",
        "        if not self.is_initialized:\n",
        "            return False\n",
        "        try:\n",
        "            self._client.delete_collection(self.config.collection_name)\n",
        "            self._collection = None\n",
        "            self._initialized = False\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "# Test the vector store\n",
        "print(\"=\" * 60)\n",
        "print(\"VECTOR STORE TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test VectorStoreConfig\n",
        "config = VectorStoreConfig()\n",
        "print(f\"\\n[OK] VectorStoreConfig created:\")\n",
        "print(f\"   - Collection: {config.collection_name}\")\n",
        "print(f\"   - Embedding model: {config.embedding_model}\")\n",
        "print(f\"   - Persist directory: {config.persist_directory}\")\n",
        "\n",
        "# Test VectorStore initialization\n",
        "store = VectorStore(config)\n",
        "print(f\"\\n[OK] VectorStore created:\")\n",
        "print(f\"   - Initialized: {store.is_initialized}\")\n",
        "\n",
        "# Try to initialize\n",
        "init_success = store.initialize()\n",
        "print(f\"\\n[OK] VectorStore initialization:\")\n",
        "print(f\"   - Success: {init_success}\")\n",
        "print(f\"   - Is initialized: {store.is_initialized}\")\n",
        "\n",
        "if store.is_initialized:\n",
        "    # Test adding documents\n",
        "    test_docs = [\n",
        "        \"Revenue increased by 15% year over year.\",\n",
        "        \"Cost of goods sold remained stable.\",\n",
        "        \"Inventory turnover improved significantly.\"\n",
        "    ]\n",
        "    add_success = store.add_documents(\n",
        "        documents=test_docs,\n",
        "        metadatas=[{\"source\": \"test\", \"idx\": i} for i in range(len(test_docs))]\n",
        "    )\n",
        "    print(f\"\\n[OK] Document addition:\")\n",
        "    print(f\"   - Added: {add_success}\")\n",
        "    print(f\"   - Document count: {store.count()}\")\n",
        "\n",
        "    # Test query\n",
        "    results = store.query(\"revenue growth\", n_results=2)\n",
        "    print(f\"\\n[OK] Query test:\")\n",
        "    print(f\"   - Results returned: {len(results.get('documents', []))}\")\n",
        "    if results.get('documents'):\n",
        "        print(f\"   - Top result: {results['documents'][0][:50]}...\")\n",
        "\n",
        "    # Test persistence\n",
        "    persist_success = store.persist()\n",
        "    print(f\"\\n[OK] Persistence:\")\n",
        "    print(f\"   - Persisted: {persist_success}\")\n",
        "else:\n",
        "    print(\"\\n[SKIP] Document operations (ChromaDB not available)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8LgDY-lr7kl"
      },
      "source": [
        "## Section 5: Module 3 - Graph Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "th4bkosQr7km"
      },
      "outputs": [],
      "source": [
        "# Graph Service\n",
        "# TODO: Implement graph analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHHBivb_r7ko"
      },
      "source": [
        "## Section 6: Module 4 - Substitution Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5BlFoqqMr7ko"
      },
      "outputs": [],
      "source": [
        "# Substitution Algorithm\n",
        "# TODO: Implement AEM/REM detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwR0eLDur7kp"
      },
      "source": [
        "## Section 7: Module 5 - Output Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_Pc0OJUOr7kp"
      },
      "outputs": [],
      "source": [
        "# Output Generation\n",
        "# TODO: Implement report generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKtH1Si9r7kq"
      },
      "source": [
        "## Section 8: Main Analyzer Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JA5D123kr7kq"
      },
      "outputs": [],
      "source": [
        "# Main Pipeline\n",
        "# TODO: Implement ARSVGAnalyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yihzA-VGr7kq"
      },
      "source": [
        "## Section 9: Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7yNkKbLgr7kr"
      },
      "outputs": [],
      "source": [
        "# Gradio UI\n",
        "# TODO: Implement UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5ngcfKOr7kr"
      },
      "source": [
        "## Section 10: Demo and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_9xsDV7Nr7kr"
      },
      "outputs": [],
      "source": [
        "# Demo and Testing\n",
        "# TODO: Implement demo functions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
